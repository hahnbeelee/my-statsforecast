{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> The `core.StatsForecast` class allows you to efficiently fit multiple `StatsForecast` models for large sets of time series. It operates with pandas DataFrame `df` that identifies individual series and datestamps with the `unique_id` and `ds` columns, and the `y` column denotes the target time series variable. To assist development, we declare useful datasets that we use throughout all `StatsForecast`'s unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from scipy.stats import norm\n",
    "\n",
    "from utilsforecast.compat import DataFrame\n",
    "from utilsforecast.data import generate_series as utils_generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Global variables\n",
    "NOGIL = bool(os.getenv('NIXTLA_NUMBA_RELEASE_GIL', ''))\n",
    "LEGACY_CACHE = bool(os.getenv('NUMBA_CACHE', ''))\n",
    "if LEGACY_CACHE:\n",
    "    warnings.warn(\n",
    "        'The NUMBA_CACHE environment variable has been renamed to NIXTLA_NUMBA_CACHE. '\n",
    "        'Please set that one instead.',\n",
    "        DeprecationWarning,\n",
    "    )\n",
    "CACHE = bool(os.getenv('NIXTLA_NUMBA_CACHE', '')) or LEGACY_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Synthetic Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_series(n_series: int,\n",
    "                    freq: str = 'D',\n",
    "                    min_length: int = 50,\n",
    "                    max_length: int = 500,\n",
    "                    n_static_features: int = 0,\n",
    "                    equal_ends: bool = False,\n",
    "                    engine:str = 'pandas', \n",
    "                    seed: int = 0) -> DataFrame:\n",
    "    \"\"\"Generate Synthetic Panel Series.\n",
    "\n",
    "    Generates `n_series` of frequency `freq` of different lengths in the interval [`min_length`, `max_length`].\n",
    "    If `n_static_features > 0`, then each series gets static features with random values.\n",
    "    If `equal_ends == True` then all series end at the same date.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `n_series`: int, number of series for synthetic panel.<br>\n",
    "    `min_length`: int, minimal length of synthetic panel's series.<br>\n",
    "    `max_length`: int, minimal length of synthetic panel's series.<br>\n",
    "    `n_static_features`: int, default=0, number of static exogenous variables for synthetic panel's series.<br>\n",
    "    `equal_ends`: bool, if True, series finish in the same date stamp `ds`.<br>\n",
    "    `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "    `engine`: str, engine to be used in DataFrame construction; NOTE: index does not exist in polars DataFrame\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `freq`: pandas.DataFrame | polars.DataFrame, synthetic panel with columns [`unique_id`, `ds`, `y`] and exogenous.\n",
    "    \"\"\"\n",
    "    return utils_generate_series(\n",
    "        n_series=n_series,\n",
    "        freq=freq,\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        n_static_features=n_static_features,\n",
    "        equal_ends=equal_ends,\n",
    "        engine=engine,\n",
    "        seed=seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(generate_series, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_panel = generate_series(n_series=2)\n",
    "synthetic_panel.groupby('unique_id').head(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AirPassengers Data\n",
    "\n",
    "The classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.\n",
    "\n",
    "It has been used as a reference on several forecasting libraries, since it is a series that shows clear trends and seasonalities it offers a nice opportunity to quickly showcase a model's predictions performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AirPassengers = np.array([112., 118., 132., 129., 121., 135., 148., 148., 136., 119., 104.,\n",
    "                          118., 115., 126., 141., 135., 125., 149., 170., 170., 158., 133.,\n",
    "                          114., 140., 145., 150., 178., 163., 172., 178., 199., 199., 184.,\n",
    "                          162., 146., 166., 171., 180., 193., 181., 183., 218., 230., 242.,\n",
    "                          209., 191., 172., 194., 196., 196., 236., 235., 229., 243., 264.,\n",
    "                          272., 237., 211., 180., 201., 204., 188., 235., 227., 234., 264.,\n",
    "                          302., 293., 259., 229., 203., 229., 242., 233., 267., 269., 270.,\n",
    "                          315., 364., 347., 312., 274., 237., 278., 284., 277., 317., 313.,\n",
    "                          318., 374., 413., 405., 355., 306., 271., 306., 315., 301., 356.,\n",
    "                          348., 355., 422., 465., 467., 404., 347., 305., 336., 340., 318.,\n",
    "                          362., 348., 363., 435., 491., 505., 404., 359., 310., 337., 360.,\n",
    "                          342., 406., 396., 420., 472., 548., 559., 463., 407., 362., 405.,\n",
    "                          417., 391., 419., 461., 472., 535., 622., 606., 508., 461., 390.,\n",
    "                          432.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AirPassengersDF = pd.DataFrame({'unique_id': np.ones(len(AirPassengers)),\n",
    "                                'ds': pd.date_range(start='1949-01-01',\n",
    "                                                    periods=len(AirPassengers), freq='M'),\n",
    "                                'y': AirPassengers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.utils import AirPassengersDF\n",
    "\n",
    "AirPassengersDF.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to plot the ARIMA predictions, and the prediction intervals.\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = AirPassengersDF.set_index('ds')\n",
    "\n",
    "plot_df[['y']].plot(ax=ax, linewidth=2)\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "@njit(nogil=NOGIL, cache=CACHE)\n",
    "def _repeat_val_seas(season_vals: np.ndarray, h: int, season_length: int):\n",
    "    out = np.empty(h, np.float32)\n",
    "    for i in range(h):\n",
    "        out[i] = season_vals[i % season_length]\n",
    "    return out\n",
    "\n",
    "@njit(nogil=NOGIL, cache=CACHE)\n",
    "def _seasonal_naive(\n",
    "        y: np.ndarray, # time series\n",
    "        h: int, # forecasting horizon\n",
    "        fitted: bool, #fitted values\n",
    "        season_length: int, # season length\n",
    "    ): \n",
    "    if y.size < season_length:\n",
    "        return {'mean': np.full(h, np.nan, np.float32)}\n",
    "    n = y.size\n",
    "    season_vals = np.empty(season_length, np.float32)\n",
    "    fitted_vals = np.full(y.size, np.nan, np.float32)\n",
    "    for i in range(season_length):\n",
    "        s_naive = _naive(y[(i +  n % season_length)::season_length], h=1, fitted=fitted)\n",
    "        season_vals[i] = s_naive['mean'].item()\n",
    "        if fitted:\n",
    "            fitted_vals[(i +  n % season_length)::season_length] = s_naive['fitted']\n",
    "    out = _repeat_val_seas(season_vals=season_vals, h=h, season_length=season_length)\n",
    "    fcst = {'mean': out}\n",
    "    if fitted:\n",
    "        fcst['fitted'] = fitted_vals[-n:]\n",
    "    return fcst\n",
    "\n",
    "@njit(nogil=NOGIL, cache=CACHE)\n",
    "def _repeat_val(val: float, h: int):\n",
    "    return np.full(h, val, np.float32)\n",
    "\n",
    "@njit(nogil=NOGIL, cache=CACHE)\n",
    "def _naive(\n",
    "        y: np.ndarray, # time series\n",
    "        h: int, # forecasting horizon\n",
    "        fitted: bool, # fitted values\n",
    "    ): \n",
    "    mean = _repeat_val(val=y[-1], h=h)\n",
    "    if fitted:\n",
    "        fitted_vals = np.full(y.size, np.nan, np.float32)\n",
    "        fitted_vals[1:] = np.roll(y, 1)[1:]\n",
    "        return {'mean': mean, 'fitted': fitted_vals}\n",
    "    return {'mean': mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test seasonal naive\n",
    "y = np.array([0.50187596, 0.40536128, 0.33436676, 0.27868117, 0.25251294,\n",
    "       0.18961286, 0.07082107, 2.58699709, 3.06466854, 2.25150509,\n",
    "       1.33027107, 0.73332616, 0.50187596, 0.40536128, 0.33436676,\n",
    "       0.27868117, 0.25251294, 0.18961286, 0.07082107, 2.58699709,\n",
    "       3.06466854, 2.25150509, 1.33027107, 0.73332616, 0.50187596,\n",
    "       0.40536128, 0.33436676, 0.27868117, 0.25251294, 0.18961286,\n",
    "       0.07082107, 2.58699709, 3.06466854, 2.25150509, 1.33027107,\n",
    "       0.73332616, 0.50187596, 0.40536128, 0.33436676, 0.27868117,\n",
    "       0.25251294, 0.18961286, 0.07082107, 2.58699709, 3.06466854,\n",
    "       2.25150509, 1.33027107, 0.73332616, 0.50187596, 0.40536128,\n",
    "       0.33436676, 0.27868117, 0.25251294, 0.18961286, 0.07082107,\n",
    "       2.58699709, 3.06466854, 2.25150509, 1.33027107, 0.73332616,\n",
    "       0.50187596, 0.40536128, 0.33436676, 0.27868117, 0.25251294,\n",
    "       0.18961286, 0.07082107, 2.58699709, 3.06466854, 2.25150509,\n",
    "       1.33027107, 0.73332616, 0.50187596, 0.40536128, 0.33436676,\n",
    "       0.27868117, 0.25251294, 0.18961286, 0.07082107, 2.58699709,\n",
    "       3.06466854, 2.25150509, 1.33027107, 0.73332616, 0.50187596,\n",
    "       0.40536128, 0.33436676, 0.27868117, 0.25251294, 0.18961286,\n",
    "       0.07082107, 2.58699709, 3.06466854, 2.25150509, 1.33027107,\n",
    "       0.73332616, 0.50187596, 0.40536128, 0.33436676, 0.27868117,\n",
    "       0.25251294, 0.18961286, 0.07082107, 2.58699709, 3.06466854,\n",
    "       2.25150509, 1.33027107, 0.73332616, 0.50187596, 0.40536128,\n",
    "       0.33436676, 0.27868117, 0.25251294, 0.18961286, 0.07082107,\n",
    "       2.58699709, 3.06466854, 2.25150509, 1.33027107, 0.73332616,\n",
    "       0.50187596, 0.40536128, 0.33436676, 0.27868117, 0.25251294,\n",
    "       0.18961286])\n",
    "seas_naive_fcst = dict(_seasonal_naive(y=y, h=12, season_length=12, fitted=True))['mean']\n",
    "np.testing.assert_array_almost_equal(seas_naive_fcst, y[-12:])\n",
    "\n",
    "\n",
    "y = np.array([0.05293832, 0.10395079, 0.25626143, 0.61529232, 1.08816604,\n",
    "       0.54493457, 0.43415014, 0.47676606, 5.32806397, 3.00553563,\n",
    "       0.04473598, 0.04920475, 0.05293832, 0.10395079, 0.25626143,\n",
    "       0.61529232, 1.08816604, 0.54493457, 0.43415014, 0.47676606,\n",
    "       5.32806397, 3.00553563, 0.04473598, 0.04920475, 0.05293832,\n",
    "       0.10395079, 0.25626143, 0.61529232, 1.08816604, 0.54493457,\n",
    "       0.43415014, 0.47676606, 5.32806397, 3.00553563, 0.04473598,\n",
    "       0.04920475, 0.05293832, 0.10395079, 0.25626143, 0.61529232,\n",
    "       1.08816604, 0.54493457, 0.43415014, 0.47676606, 5.32806397,\n",
    "       3.00553563, 0.04473598, 0.04920475, 0.05293832, 0.10395079,\n",
    "       0.25626143, 0.61529232, 1.08816604])\n",
    "seas_naive_fcst = dict(_seasonal_naive(y=y, h=12, season_length=12, fitted=True))['mean']\n",
    "np.testing.assert_array_almost_equal(seas_naive_fcst, y[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Functions used for calculating prediction intervals \n",
    "def _quantiles(level): \n",
    "    level = np.asarray(level)\n",
    "    z = norm.ppf(0.5+level/200)   \n",
    "    return z\n",
    "\n",
    "def _calculate_intervals(out, level, h, sigmah):\n",
    "    z = _quantiles(np.asarray(level))\n",
    "    zz = np.repeat(z, h)\n",
    "    zz = zz.reshape(z.shape[0], h)\n",
    "    lower = out['mean'] - zz * sigmah\n",
    "    upper = out['mean'] + zz * sigmah\n",
    "    pred_int = {**{f'lo-{lv}': lower[i] for i, lv in enumerate(level)}, \n",
    "                **{f'hi-{lv}': upper[i] for i, lv in enumerate(level)}}    \n",
    "    return pred_int\n",
    "\n",
    "def _calculate_sigma(residuals, n): \n",
    "    sigma = np.nansum(residuals ** 2) \n",
    "    sigma = sigma / n\n",
    "    sigma = np.sqrt(sigma)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ConformalIntervals:\n",
    "    \"\"\"Class for storing conformal intervals metadata information.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_windows: int = 2,\n",
    "        h: int = 1,\n",
    "        method: str = \"conformal_distribution\",\n",
    "    ):\n",
    "        if n_windows < 2:\n",
    "            raise ValueError(\n",
    "                \"You need at least two windows to compute conformal intervals\"\n",
    "            )\n",
    "        allowed_methods = [\"conformal_distribution\"]\n",
    "        if method not in allowed_methods:\n",
    "            raise ValueError(f\"method must be one of {allowed_methods}\")\n",
    "        self.n_windows = n_windows\n",
    "        self.h = h\n",
    "        self.method = method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
